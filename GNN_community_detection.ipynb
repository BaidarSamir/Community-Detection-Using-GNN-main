{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('visuals', exist_ok=True)\n",
        "\n",
        "def save_fig(fname):\n",
        "    \"\"\"Save the current matplotlib figure into the visuals folder and close it.\"\"\"\n",
        "    os.makedirs('visuals', exist_ok=True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join('visuals', fname))\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd4F2_qpaggX"
      },
      "source": [
        "## CSI 4900- Community detection using GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg4p-EM0Vb17"
      },
      "source": [
        "<h4>Importing Necessary Libraries</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.2.6)\n",
            "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (3.9.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (3.4.2)\n",
            "Requirement already satisfied: torch in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.9.0)\n",
            "Requirement already satisfied: torch_geometric in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch_geometric) (3.13.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->torch_geometric) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "# (Optional, for Colab – run once)\n",
        "!pip install numpy pandas matplotlib networkx torch torch_geometric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgvRK3tpIDNi",
        "outputId": "51169a45-95e8-4a49-e0dd-f5ebf6474d5b"
      },
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import json\n",
        "import collections\n",
        "\n",
        "# Third-party\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import networkx as nx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BIKz6Z5Ut54"
      },
      "source": [
        "### Pre-processing the dataset\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we load the raw feature, edge, and label files from GitHub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "QsSOg60FIM6x"
      },
      "outputs": [],
      "source": [
        "json_url = \"https://raw.githubusercontent.com/mehtameet12/GNN_Dataset/main/musae_git_features.json\"\n",
        "edges_url = \"https://raw.githubusercontent.com/mehtameet12/GNN_Dataset/main/musae_git_edges.csv\"\n",
        "target_url = \"https://raw.githubusercontent.com/mehtameet12/GNN_Dataset/main/musae_git_target.csv\"\n",
        "\n",
        "# Fetch JSON features\n",
        "response = requests.get(json_url)\n",
        "response.raise_for_status()  # raises a clear HTTPError if it fails\n",
        "data_raw = json.loads(response.text)\n",
        "\n",
        "# Fetch edges and targets\n",
        "edges = pd.read_csv(edges_url)\n",
        "target_df = pd.read_csv(target_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "l1Yyvc4Q53iv",
        "outputId": "5626f9bd-fb5e-462b-e475-718ad603c3b9",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows of the target (labels) dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>ml_target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Eiryyy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>shawflying</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>JpMCarrilho</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>SuhwanCha</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>sunilangadi2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id          name  ml_target\n",
              "0   0        Eiryyy          0\n",
              "1   1    shawflying          0\n",
              "2   2   JpMCarrilho          1\n",
              "3   3     SuhwanCha          0\n",
              "4   4  sunilangadi2          1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"First 5 rows of the target (labels) dataset:\")\n",
        "display(target_df.head())  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "5qCq-3gG53iv",
        "outputId": "4739a3ad-34c6-4e91-9ca9-24b4a25b67c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last 5 rows of the target (labels) dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>ml_target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37695</th>\n",
              "      <td>37695</td>\n",
              "      <td>shawnwanderson</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37696</th>\n",
              "      <td>37696</td>\n",
              "      <td>kris-ipeh</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37697</th>\n",
              "      <td>37697</td>\n",
              "      <td>qpautrat</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37698</th>\n",
              "      <td>37698</td>\n",
              "      <td>Injabie3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37699</th>\n",
              "      <td>37699</td>\n",
              "      <td>caseycavanagh</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id            name  ml_target\n",
              "37695  37695  shawnwanderson          1\n",
              "37696  37696       kris-ipeh          0\n",
              "37697  37697        qpautrat          0\n",
              "37698  37698        Injabie3          1\n",
              "37699  37699   caseycavanagh          0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Last 5 rows of the target (labels) dataset:\")\n",
        "display(target_df.tail())  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8kLXN9cWJuZ"
      },
      "source": [
        "### Processing the dataset\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "nR-ojoIjTGUJ"
      },
      "outputs": [],
      "source": [
        "feats = []\n",
        "feat_counts = []\n",
        "\n",
        "for i in range(len(data_raw)):\n",
        "    feat_list = data_raw[str(i)]\n",
        "    feat_counts.append(len(feat_list))\n",
        "    feats.extend(feat_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "PXboUx4ITOeB"
      },
      "outputs": [],
      "source": [
        "#We are counting the frequency of each feature and storing it in a dictionary called counter\n",
        "counter=collections.Counter(feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf5XC0i1W52m"
      },
      "source": [
        "<h4>Data Analysis</h4>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNIHQo3GHCqP",
        "outputId": "9fbc1b8e-75cb-4301-a3ab-b2486e226cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features for the first 5 nodes:\n",
            "\n",
            "[19, 17, 19, 15, 19]\n"
          ]
        }
      ],
      "source": [
        "print('Number of features for the first 5 nodes:\\n')\n",
        "print(feat_counts[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "GnyVSmf4XDLw",
        "outputId": "7be84f30-61ac-43fd-8455-440ca8244dc3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "top_features = [feature for feature, count in counter.most_common(10)]\n",
        "top_feature_counts = [count for feature, count in counter.most_common(10)]\n",
        "\n",
        "top_features_df = pd.DataFrame({'Features': top_features, 'Counts': top_feature_counts})\n",
        "\n",
        "\n",
        "top_features_df = top_features_df.sort_values(by='Counts', ascending=False)\n",
        "\n",
        "# Increase the size of the heatmap figure\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a heatmap with feature counts\n",
        "sns.heatmap(top_features_df[['Counts']].T, cmap='viridis', annot=True, fmt='d', annot_kws={\"size\": 12}, cbar=False)\n",
        "\n",
        "# Set the x-axis labels to feature names\n",
        "plt.xticks(ticks=[i + 0.5 for i in range(10)], labels=top_features, rotation=45, ha='right')\n",
        "\n",
        "plt.title('Top 10 Most Occurring Features Heatmap')\n",
        "save_fig('top_features_heatmap.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLHK-3X8Xbb8"
      },
      "source": [
        "<h4>Data Encoding</h4>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "TC0JGS64TTsV"
      },
      "outputs": [],
      "source": [
        "#Encoding the Data\n",
        "def encode_data(light=False,n=60):\n",
        "  #Trying to work with only 60 nodes due to limited computer resources\n",
        "    if light==True:\n",
        "        nodes_included=n\n",
        "    elif light==False:\n",
        "        nodes_included=len(data_raw)\n",
        "\n",
        "  #data_encoded dictionary will store all a list of all 4005 features for every (37,700) nodes filled with 1's and 0's depending on the presence of each feature\n",
        "    data_encoded={}\n",
        "    for i in range(nodes_included):#\n",
        "        one_hot_feat=np.array([0]*(max(feats)+1))\n",
        "        this_feat=data_raw[str(i)]\n",
        "        one_hot_feat[this_feat]=1\n",
        "        data_encoded[str(i)]=list(one_hot_feat)\n",
        "\n",
        "  #Sice the value (list) of each key (node) is 4005 elements long, mostly containing 1's and 0's, we are creating a sparse matrix\n",
        "    if light==True:\n",
        "        sparse_feat_matrix=np.zeros((1,max(feats)+1))\n",
        "        for j in range(nodes_included):\n",
        "            temp=np.array(data_encoded[str(j)]).reshape(1,-1)\n",
        "            sparse_feat_matrix=np.concatenate((sparse_feat_matrix,temp),axis=0)\n",
        "        sparse_feat_matrix=sparse_feat_matrix[1:,:]\n",
        "        return(data_encoded,sparse_feat_matrix)\n",
        "    elif light==False:\n",
        "        return(data_encoded, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0-pNUUYX9Ox"
      },
      "source": [
        "<h5>Sparse Matrix plotting the first 550 features in the first 100 nodes</h5>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "BUlKxgwNTWga",
        "outputId": "1d28b72a-75f6-4f9c-aaac-ff75aebeb953"
      },
      "outputs": [],
      "source": [
        "#since we cannot fit all 4005 features in the window, we are showing the first 550 feautures of the first 100 nodes by passing the value to the encoded function which will return a sparse matrix\n",
        "data_encoded_vis,sparse_feat_matrix_vis=encode_data(light=True,n=100)\n",
        "plt.figure(figsize=(50,50));\n",
        "plt.imshow(sparse_feat_matrix_vis[:,:550],cmap='Greys');\n",
        "plt.grid()\n",
        "save_fig('sparse_features.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCFvge5YYZjg"
      },
      "source": [
        "<h4> Constructing a Graph </h4>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "cJ3qZbNkTZOj"
      },
      "outputs": [],
      "source": [
        "def construct_graph(data_encoded, light=False):\n",
        "    # Extract the node features from the input data and convert them to a tensor.\n",
        "    node_features_list = list(data_encoded.values())\n",
        "    node_features = torch.tensor(node_features_list)\n",
        "\n",
        "    # Extract node labels from the 'target_df' dataframe and convert them to a tensor.\n",
        "    node_labels = torch.tensor(target_df['ml_target'].values)\n",
        "\n",
        "    # Prepare the edge data from the 'edges' variable and create edge tensors.\n",
        "    edges_list = edges.values.tolist()\n",
        "    edge_index01 = torch.tensor(edges_list, dtype=torch.long).T\n",
        "    edge_index02 = torch.zeros(edge_index01.shape, dtype=torch.long)\n",
        "\n",
        "    # Create reverse edges by swapping source and target indices.\n",
        "    edge_index02[0, :] = edge_index01[1, :]\n",
        "    edge_index02[1, :] = edge_index01[0, :]\n",
        "\n",
        "    # Concatenate both the original and reverse edges to create a combined edge index.\n",
        "    edge_index0 = torch.cat((edge_index01, edge_index02), axis=1)\n",
        "\n",
        "    # Create a PyTorch Geometric 'Data' object representing the graph with node features, labels, and edges.\n",
        "    g = Data(x=node_features, y=node_labels, edge_index=edge_index0)\n",
        "\n",
        "    # Create a \"light\" version of the graph with reduced dimensions.\n",
        "    g_light = Data(x=node_features[:, 0:2], y=node_labels, edge_index=edge_index0[:, :55])\n",
        "\n",
        "    # If the 'light' parameter is True, return the light version; otherwise, return the full graph.\n",
        "    if light:\n",
        "        return g_light\n",
        "    else:\n",
        "        return g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "2R8pxKyITcyr"
      },
      "outputs": [],
      "source": [
        "def draw_graph(data0):\n",
        "    # Check if the graph has more than 100 nodes.\n",
        "    if data0.num_nodes > 100:\n",
        "        # If it's a large graph, print a message and exit without plotting.\n",
        "        print(\"This is a big graph, cannot plot...\")\n",
        "        return\n",
        "\n",
        "    else:\n",
        "        # Convert the input PyTorch Geometric 'Data' object to a NetworkX graph.\n",
        "        data_nx = to_networkx(data0)\n",
        "\n",
        "        # Extract node colors from the 'data0' object based on node labels.\n",
        "        node_colors = data0.y[list(data_nx.nodes)]\n",
        "\n",
        "        # Compute the positions of nodes using the spring layout algorithm.\n",
        "        pos = nx.spring_layout(data_nx, scale=1)\n",
        "\n",
        "        # Create a Matplotlib figure for the graph visualization.\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Use NetworkX to draw the graph with various visualization settings.\n",
        "        nx.draw(data_nx, pos, cmap=plt.get_cmap('Set1'),\n",
        "                node_color=node_colors, node_size=600, connectionstyle=\"angle3\",\n",
        "                width=1, with_labels=True, edge_color='k', arrowstyle=\"-\")\n",
        "        save_fig('graph_sample.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "PSAbV5YrTgiF",
        "outputId": "4e59bd9a-d0fd-494c-f5fe-b3f9e6f54aac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_18096\\1330896778.py:7: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
            "  plt.tight_layout()\n"
          ]
        }
      ],
      "source": [
        "# Constructing the graph with 'g_light' representing the connection of edges.\n",
        "# The gray color represents Machine Learning (ML) while the Red represents Web Development.\n",
        "\n",
        "# The 'light' version is suitable for visualization with reduced dimensions.\n",
        "g_sample = construct_graph(data_encoded=data_encoded_vis, light=True)\n",
        "\n",
        "# Visualize the 'g_sample' graph.\n",
        "draw_graph(g_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation of Graph Properties\n",
        "\n",
        "**Key Findings:**\n",
        "\n",
        "1. **Homophily Ratio**: Measures the tendency of developers to connect with others in the same community (ML vs. Web). A high ratio (>0.6) indicates strong community structure, which is ideal for GNN-based learning because node labels are correlated with network topology.\n",
        "\n",
        "2. **Degree Distribution**: Shows how connections are distributed across developers. A power-law distribution (common in social networks) indicates the presence of \"hub\" developers with many connections, while most developers have few connections. This heterogeneity is captured well by graph convolutions.\n",
        "\n",
        "3. **Clustering Coefficient**: Quantifies the prevalence of triangular relationships (friend-of-friend connections). High clustering (>0.3) suggests tight-knit communities where developers form closed groups, strengthening the community signal for GNNs.\n",
        "\n",
        "4. **Graph Density**: The network is extremely sparse, with only a tiny fraction of possible edges present. This sparsity makes GNNs computationally efficient compared to dense neural networks.\n",
        "\n",
        "5. **Assortativity**: Positive assortativity means highly-connected developers tend to connect with other well-connected developers, forming an \"elite core.\" Negative assortativity suggests hubs bridge different communities.\n",
        "\n",
        "6. **Connected Components**: A large giant component (>95% of nodes) ensures that message-passing can propagate information across most of the network during GNN training.\n",
        "\n",
        "**Why This Matters for GNNs**: High homophily combined with significant clustering creates a strong \"network effect\" where knowing a developer's neighbors improves prediction accuracy. This explains why our GNN (86.4% test accuracy) outperforms feature-only models like Logistic Regression (83.4%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Analyze the Full Graph\n",
        "\n",
        "Now we'll compute the properties on the full 37K-node graph. Note: The graph `g` needs to be constructed first (see cells below in \"GNN Model Construction\" section)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_graph_properties(graph_props, data):\n",
        "    \"\"\"\n",
        "    Create a comprehensive visualization of graph structural properties.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    fig.suptitle('GitHub Developer Network: Structural Properties Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. HOMOPHILY BAR CHART\n",
        "    ax1 = axes[0, 0]\n",
        "    homophily_data = [graph_props['homophily'], 1 - graph_props['homophily']]\n",
        "    colors_homo = ['#2ecc71', '#e74c3c']\n",
        "    ax1.bar(['Same Label', 'Different Label'], homophily_data, color=colors_homo, alpha=0.7, edgecolor='black')\n",
        "    ax1.set_ylabel('Fraction of Edges', fontsize=11)\n",
        "    ax1.set_title(f'Homophily Ratio: {graph_props[\"homophily\"]:.3f}', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylim([0, 1])\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add annotation\n",
        "    ax1.text(0.5, 0.95, 'High homophily → GNNs effective', \n",
        "             transform=ax1.transAxes, ha='center', va='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), fontsize=9)\n",
        "    \n",
        "    # 2. DEGREE DISTRIBUTION\n",
        "    ax2 = axes[0, 1]\n",
        "    degrees = graph_props['degrees']\n",
        "    ax2.hist(degrees, bins=50, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "    ax2.set_xlabel('Node Degree', fontsize=11)\n",
        "    ax2.set_ylabel('Frequency', fontsize=11)\n",
        "    ax2.set_title(f'Degree Distribution (avg={graph_props[\"avg_degree\"]:.1f})', fontsize=12, fontweight='bold')\n",
        "    ax2.set_yscale('log')\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    # 3. DEGREE STATISTICS BOX\n",
        "    ax3 = axes[0, 2]\n",
        "    ax3.axis('off')\n",
        "    stats_text = f\"\"\"\n",
        "    Graph Connectivity Statistics\n",
        "    \n",
        "    Total Nodes: {len(data.y):,}\n",
        "    Total Edges: {data.edge_index.shape[1]//2:,}\n",
        "    \n",
        "    Average Degree: {graph_props['avg_degree']:.2f}\n",
        "    Median Degree: {graph_props['median_degree']:.1f}\n",
        "    Max Degree: {graph_props['max_degree']}\n",
        "    \n",
        "    Graph Density: {graph_props['density']:.6f}\n",
        "    (Very sparse network)\n",
        "    \"\"\"\n",
        "    ax3.text(0.1, 0.5, stats_text, transform=ax3.transAxes, \n",
        "             fontsize=11, verticalalignment='center',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3),\n",
        "             family='monospace')\n",
        "    \n",
        "    # 4. CLUSTERING COEFFICIENT\n",
        "    ax4 = axes[1, 0]\n",
        "    clustering_visual = [graph_props['avg_clustering'], 1 - graph_props['avg_clustering']]\n",
        "    ax4.pie(clustering_visual, labels=['Triangular', 'Non-triangular'], \n",
        "            autopct='%1.1f%%', colors=['#9b59b6', '#ecf0f1'], startangle=90,\n",
        "            explode=(0.05, 0))\n",
        "    ax4.set_title(f'Avg Clustering Coeff: {graph_props[\"avg_clustering\"]:.3f}', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    \n",
        "    # 5. ASSORTATIVITY\n",
        "    ax5 = axes[1, 1]\n",
        "    assortativity = graph_props['assortativity']\n",
        "    ax5.barh(['Degree\\nAssortativity'], [assortativity], color='#e67e22', alpha=0.7, edgecolor='black')\n",
        "    ax5.set_xlim([-1, 1])\n",
        "    ax5.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
        "    ax5.set_xlabel('Coefficient', fontsize=11)\n",
        "    ax5.set_title(f'Network Assortativity: {assortativity:.3f}', fontsize=12, fontweight='bold')\n",
        "    ax5.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Add interpretation\n",
        "    if assortativity > 0:\n",
        "        interpretation = 'Assortative: Hubs connect to hubs'\n",
        "    else:\n",
        "        interpretation = 'Disassortative: Hubs connect to periphery'\n",
        "    ax5.text(0.5, 0.15, interpretation, transform=ax5.transAxes, ha='center',\n",
        "             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3), fontsize=9)\n",
        "    \n",
        "    # 6. CONNECTED COMPONENTS\n",
        "    ax6 = axes[1, 2]\n",
        "    component_data = [graph_props['largest_cc_fraction'], 1 - graph_props['largest_cc_fraction']]\n",
        "    colors_cc = ['#1abc9c', '#95a5a6']\n",
        "    wedges, texts, autotexts = ax6.pie(component_data, \n",
        "                                         labels=['Giant Component', 'Other Components'],\n",
        "                                         autopct='%1.1f%%', colors=colors_cc, startangle=90,\n",
        "                                         explode=(0.05, 0))\n",
        "    ax6.set_title(f'Connectivity: {graph_props[\"num_components\"]} components', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_fig('graph_properties_analysis.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_graph_properties(data):\n",
        "    \"\"\"\n",
        "    Compute key structural properties of the graph that influence GNN performance.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary containing graph metrics\n",
        "    \"\"\"\n",
        "    # Convert PyG Data to NetworkX (undirected for structural analysis)\n",
        "    G = to_networkx(data, to_undirected=True)\n",
        "    \n",
        "    print(\"Computing graph properties...\")\n",
        "    print(f\"Total nodes: {G.number_of_nodes()}\")\n",
        "    print(f\"Total edges: {G.number_of_edges()}\")\n",
        "    \n",
        "    # 1. HOMOPHILY RATIO - Critical for understanding why GNNs work\n",
        "    # Measures the fraction of edges connecting nodes with the same label\n",
        "    same_label_edges = 0\n",
        "    total_edges = 0\n",
        "    \n",
        "    for u, v in G.edges():\n",
        "        if u < len(data.y) and v < len(data.y):\n",
        "            if data.y[u] == data.y[v]:\n",
        "                same_label_edges += 1\n",
        "            total_edges += 1\n",
        "    \n",
        "    homophily = same_label_edges / total_edges if total_edges > 0 else 0\n",
        "    print(f\"Homophily ratio: {homophily:.4f}\")\n",
        "    \n",
        "    # 2. GRAPH DENSITY - Sparsity measure\n",
        "    density = nx.density(G)\n",
        "    print(f\"Graph density: {density:.6f}\")\n",
        "    \n",
        "    # 3. AVERAGE CLUSTERING COEFFICIENT - Local triangle density\n",
        "    avg_clustering = nx.average_clustering(G)\n",
        "    print(f\"Average clustering coefficient: {avg_clustering:.4f}\")\n",
        "    \n",
        "    # 4. DEGREE STATISTICS\n",
        "    degrees = [d for n, d in G.degree()]\n",
        "    avg_degree = np.mean(degrees)\n",
        "    median_degree = np.median(degrees)\n",
        "    max_degree = np.max(degrees)\n",
        "    \n",
        "    print(f\"Average degree: {avg_degree:.2f}\")\n",
        "    print(f\"Median degree: {median_degree:.2f}\")\n",
        "    print(f\"Max degree: {max_degree}\")\n",
        "    \n",
        "    # 5. ASSORTATIVITY - Do high-degree nodes connect to high-degree nodes?\n",
        "    assortativity = nx.degree_assortativity_coefficient(G)\n",
        "    print(f\"Degree assortativity: {assortativity:.4f}\")\n",
        "    \n",
        "    # 6. CONNECTED COMPONENTS\n",
        "    num_components = nx.number_connected_components(G)\n",
        "    largest_cc = max(nx.connected_components(G), key=len)\n",
        "    largest_cc_size = len(largest_cc)\n",
        "    largest_cc_fraction = largest_cc_size / G.number_of_nodes()\n",
        "    \n",
        "    print(f\"Number of connected components: {num_components}\")\n",
        "    print(f\"Largest component size: {largest_cc_size} ({largest_cc_fraction*100:.2f}%)\")\n",
        "    \n",
        "    return {\n",
        "        'homophily': homophily,\n",
        "        'density': density,\n",
        "        'avg_clustering': avg_clustering,\n",
        "        'degrees': degrees,\n",
        "        'avg_degree': avg_degree,\n",
        "        'median_degree': median_degree,\n",
        "        'max_degree': max_degree,\n",
        "        'assortativity': assortativity,\n",
        "        'num_components': num_components,\n",
        "        'largest_cc_fraction': largest_cc_fraction\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Graph Structure Analysis\n",
        "---\n",
        "Before training the GNN, we analyze key structural properties of the GitHub developer network to understand why graph-based learning is effective for community detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Em1NSfw53iz"
      },
      "source": [
        "**GNN Model Construction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "dJaOU3GkTjj9"
      },
      "outputs": [],
      "source": [
        "data_encoded,_=encode_data(light=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "1vu9CuKATrT5"
      },
      "outputs": [],
      "source": [
        "g=construct_graph(data_encoded=data_encoded,light=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing graph properties...\n",
            "Total nodes: 37700\n",
            "Total edges: 289003\n",
            "Homophily ratio: 0.8453\n",
            "Graph density: 0.000407\n",
            "Homophily ratio: 0.8453\n",
            "Graph density: 0.000407\n",
            "Average clustering coefficient: 0.1675\n",
            "Average degree: 15.33\n",
            "Median degree: 6.00\n",
            "Max degree: 9458\n",
            "Average clustering coefficient: 0.1675\n",
            "Average degree: 15.33\n",
            "Median degree: 6.00\n",
            "Max degree: 9458\n",
            "Degree assortativity: -0.0752\n",
            "Number of connected components: 1\n",
            "Largest component size: 37700 (100.00%)\n",
            "Degree assortativity: -0.0752\n",
            "Number of connected components: 1\n",
            "Largest component size: 37700 (100.00%)\n"
          ]
        }
      ],
      "source": [
        "# Analyze the full graph properties\n",
        "graph_props = analyze_graph_properties(g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the properties\n",
        "visualize_graph_properties(graph_props, g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFRCvCvxTwVB",
        "outputId": "38ab498b-efa3-4bc6-9512-d1ae2ce085ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(x=[37700, 4005], edge_index=[2, 578006], y=[37700], train_mask=[37700], val_mask=[37700], test_mask=[37700])\n",
            "\n",
            "Training samples: 22620\n",
            "Validation samples: 11310\n",
            "Test samples: 3770\n"
          ]
        }
      ],
      "source": [
        "# Define the labels/targets (assuming they are in g)\n",
        "labels = g.y\n",
        "\n",
        "# Split the data into training, validation, and test sets based on the ration shown below\n",
        "train_ratio = 0.6\n",
        "val_ratio = 0.3\n",
        "test_ratio = 0.1\n",
        "\n",
        "train_idx, test_idx, train_labels, test_labels = train_test_split(\n",
        "    range(len(labels)), labels, test_size=test_ratio, random_state=42\n",
        ")\n",
        "\n",
        "train_idx, val_idx, train_labels, val_labels = train_test_split(\n",
        "    train_idx, train_labels, test_size=val_ratio / (1 - test_ratio), random_state=42\n",
        ")\n",
        "\n",
        "# Create mask tensors for training, validation, and test sets\n",
        "train_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
        "val_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
        "test_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
        "\n",
        "train_mask[train_idx] = 1\n",
        "val_mask[val_idx] = 1\n",
        "test_mask[test_idx] = 1\n",
        "\n",
        "# Assign masks to the graph\n",
        "g.train_mask = train_mask\n",
        "g.val_mask = val_mask\n",
        "g.test_mask = test_mask\n",
        "\n",
        "print(g)\n",
        "print()\n",
        "print(\"Training samples:\", torch.sum(g.train_mask).item())\n",
        "print(\"Validation samples:\", torch.sum(g.val_mask).item())\n",
        "print(\"Test samples:\", torch.sum(g.test_mask).item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "cEHDqyCETyLV"
      },
      "outputs": [],
      "source": [
        "class SocialGNN(torch.nn.Module):\n",
        "    def __init__(self,num_of_feat,f):\n",
        "        super(SocialGNN, self).__init__()\n",
        "\n",
        "        self.conv1 = GCNConv(num_of_feat, f)\n",
        "        self.conv2 = GCNConv(f, 2)\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index =  data.edge_index\n",
        "\n",
        "        x = self.conv1(x=x, edge_index=edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "wrUb3X6MU8x2"
      },
      "outputs": [],
      "source": [
        "def masked_loss(predictions, labels, mask):\n",
        "    # Use only the nodes where mask == True\n",
        "    return criterion(predictions[mask], labels[mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "3YhGoSpbVAMP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def masked_accuracy(predictions, labels, mask):\n",
        "    # Class with highest score\n",
        "    preds = predictions.argmax(dim=1)\n",
        "    # Check correctness only on masked nodes\n",
        "    correct = (preds[mask] == labels[mask]).float()\n",
        "    # Mean accuracy on that subset\n",
        "    return correct.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "XOtXCZfVVJvp"
      },
      "outputs": [],
      "source": [
        "test_list = []\n",
        "def train_social(net, data, epochs=10, initial_lr=0.01):\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=initial_lr)\n",
        "    best_accuracy = 0.0\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)  # Learning rate scheduler\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for ep in range(epochs + 1):\n",
        "        optimizer.zero_grad()\n",
        "        out = net(data)\n",
        "        loss = masked_loss(predictions=out, labels=data.y, mask=data.train_mask)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        train_accuracy = masked_accuracy(predictions=out, labels=data.y, mask=data.train_mask)\n",
        "        train_accuracies.append(train_accuracy.item())\n",
        "\n",
        "        val_loss = masked_loss(predictions=out, labels=data.y, mask=data.val_mask)\n",
        "        val_losses.append(val_loss.item())\n",
        "        val_accuracy = masked_accuracy(predictions=out, labels=data.y, mask=data.val_mask)\n",
        "        val_accuracies.append(val_accuracy.item())\n",
        "\n",
        "        test_accuracy = masked_accuracy(predictions=out, labels=data.y, mask=data.test_mask)\n",
        "        test_accuracies.append(test_accuracy.item())\n",
        "        test_list.append(test_accuracy.item())\n",
        "\n",
        "        if np.round(val_accuracy.item(), 4) > np.round(best_accuracy, 4):\n",
        "            print(\"Epoch {}/{}, Train_Loss: {:.4f}, Train_Accuracy: {:.4f}, Val_Accuracy: {:.4f}, Test_Accuracy: {:.4f}\" \n",
        "                  .format(ep + 1, epochs, loss.item(), train_accuracy.item(), val_accuracy.item(), test_accuracy.item()))\n",
        "            best_accuracy = val_accuracy\n",
        "\n",
        "        # Learning rate schedule step\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Validation Loss\")\n",
        "    plt.plot(test_losses, label=\"Test Loss\")\n",
        "    plt.legend()\n",
        "    save_fig('gnn_losses.png')\n",
        "\n",
        "    plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "    plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "    plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
        "    plt.legend()\n",
        "    save_fig('gnn_accuracies.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcBNpf1_53i0"
      },
      "source": [
        "**Running the model with Hyperparameter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SqAd3k-r53i1",
        "outputId": "a1d2741b-cb50-44a7-ca8a-266657db7f7f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Train_Loss: 0.6845, Train_Accuracy: 0.7760, Val_Accuracy: 0.7790, Test_Accuracy: 0.7761\n",
            "Epoch 4/50, Train_Loss: 0.4948, Train_Accuracy: 0.8339, Val_Accuracy: 0.8259, Test_Accuracy: 0.8313\n",
            "Epoch 4/50, Train_Loss: 0.4948, Train_Accuracy: 0.8339, Val_Accuracy: 0.8259, Test_Accuracy: 0.8313\n",
            "Epoch 5/50, Train_Loss: 0.4813, Train_Accuracy: 0.8613, Val_Accuracy: 0.8566, Test_Accuracy: 0.8568\n",
            "Epoch 5/50, Train_Loss: 0.4813, Train_Accuracy: 0.8613, Val_Accuracy: 0.8566, Test_Accuracy: 0.8568\n",
            "Epoch 10/50, Train_Loss: 0.3482, Train_Accuracy: 0.8653, Val_Accuracy: 0.8629, Test_Accuracy: 0.8538\n",
            "Epoch 10/50, Train_Loss: 0.3482, Train_Accuracy: 0.8653, Val_Accuracy: 0.8629, Test_Accuracy: 0.8538\n",
            "Epoch 11/50, Train_Loss: 0.3551, Train_Accuracy: 0.8641, Val_Accuracy: 0.8635, Test_Accuracy: 0.8520\n",
            "Epoch 11/50, Train_Loss: 0.3551, Train_Accuracy: 0.8641, Val_Accuracy: 0.8635, Test_Accuracy: 0.8520\n",
            "Epoch 12/50, Train_Loss: 0.3387, Train_Accuracy: 0.8688, Val_Accuracy: 0.8665, Test_Accuracy: 0.8554\n",
            "Epoch 12/50, Train_Loss: 0.3387, Train_Accuracy: 0.8688, Val_Accuracy: 0.8665, Test_Accuracy: 0.8554\n",
            "Epoch 16/50, Train_Loss: 0.3218, Train_Accuracy: 0.8722, Val_Accuracy: 0.8671, Test_Accuracy: 0.8592\n",
            "Epoch 16/50, Train_Loss: 0.3218, Train_Accuracy: 0.8722, Val_Accuracy: 0.8671, Test_Accuracy: 0.8592\n",
            "Epoch 17/50, Train_Loss: 0.3197, Train_Accuracy: 0.8747, Val_Accuracy: 0.8677, Test_Accuracy: 0.8576\n",
            "Epoch 17/50, Train_Loss: 0.3197, Train_Accuracy: 0.8747, Val_Accuracy: 0.8677, Test_Accuracy: 0.8576\n",
            "Epoch 18/50, Train_Loss: 0.3220, Train_Accuracy: 0.8750, Val_Accuracy: 0.8684, Test_Accuracy: 0.8568\n",
            "Epoch 18/50, Train_Loss: 0.3220, Train_Accuracy: 0.8750, Val_Accuracy: 0.8684, Test_Accuracy: 0.8568\n",
            "Epoch 19/50, Train_Loss: 0.3150, Train_Accuracy: 0.8772, Val_Accuracy: 0.8689, Test_Accuracy: 0.8610\n",
            "Epoch 19/50, Train_Loss: 0.3150, Train_Accuracy: 0.8772, Val_Accuracy: 0.8689, Test_Accuracy: 0.8610\n",
            "Epoch 32/50, Train_Loss: 0.2869, Train_Accuracy: 0.8855, Val_Accuracy: 0.8691, Test_Accuracy: 0.8629\n",
            "Epoch 32/50, Train_Loss: 0.2869, Train_Accuracy: 0.8855, Val_Accuracy: 0.8691, Test_Accuracy: 0.8629\n"
          ]
        }
      ],
      "source": [
        "num_of_feat=g.num_node_features\n",
        "net=SocialGNN(num_of_feat=num_of_feat,f=16)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "train_social(net,g,epochs=50,initial_lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvdP8UGn53i1"
      },
      "source": [
        "**Running the model with Different Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T6BERLxK53i1",
        "outputId": "bc969a7d-66e0-4d73-a8ad-da6b087c5dbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Train_Loss: 0.7010, Train_Accuracy: 0.2792, Val_Accuracy: 0.2860, Test_Accuracy: 0.2806\n",
            "Epoch 2/100, Train_Loss: 0.6240, Train_Accuracy: 0.7432, Val_Accuracy: 0.7380, Test_Accuracy: 0.7435\n",
            "Epoch 2/100, Train_Loss: 0.6240, Train_Accuracy: 0.7432, Val_Accuracy: 0.7380, Test_Accuracy: 0.7435\n",
            "Epoch 3/100, Train_Loss: 0.5726, Train_Accuracy: 0.7432, Val_Accuracy: 0.7381, Test_Accuracy: 0.7435\n",
            "Epoch 3/100, Train_Loss: 0.5726, Train_Accuracy: 0.7432, Val_Accuracy: 0.7381, Test_Accuracy: 0.7435\n",
            "Epoch 7/100, Train_Loss: 0.5133, Train_Accuracy: 0.7435, Val_Accuracy: 0.7382, Test_Accuracy: 0.7438\n",
            "Epoch 7/100, Train_Loss: 0.5133, Train_Accuracy: 0.7435, Val_Accuracy: 0.7382, Test_Accuracy: 0.7438\n",
            "Epoch 8/100, Train_Loss: 0.5061, Train_Accuracy: 0.7437, Val_Accuracy: 0.7385, Test_Accuracy: 0.7438\n",
            "Epoch 8/100, Train_Loss: 0.5061, Train_Accuracy: 0.7437, Val_Accuracy: 0.7385, Test_Accuracy: 0.7438\n",
            "Epoch 9/100, Train_Loss: 0.4942, Train_Accuracy: 0.7438, Val_Accuracy: 0.7386, Test_Accuracy: 0.7446\n",
            "Epoch 9/100, Train_Loss: 0.4942, Train_Accuracy: 0.7438, Val_Accuracy: 0.7386, Test_Accuracy: 0.7446\n",
            "Epoch 10/100, Train_Loss: 0.4797, Train_Accuracy: 0.7441, Val_Accuracy: 0.7387, Test_Accuracy: 0.7454\n",
            "Epoch 10/100, Train_Loss: 0.4797, Train_Accuracy: 0.7441, Val_Accuracy: 0.7387, Test_Accuracy: 0.7454\n",
            "Epoch 11/100, Train_Loss: 0.4656, Train_Accuracy: 0.7448, Val_Accuracy: 0.7401, Test_Accuracy: 0.7456\n",
            "Epoch 11/100, Train_Loss: 0.4656, Train_Accuracy: 0.7448, Val_Accuracy: 0.7401, Test_Accuracy: 0.7456\n",
            "Epoch 12/100, Train_Loss: 0.4548, Train_Accuracy: 0.7467, Val_Accuracy: 0.7415, Test_Accuracy: 0.7475\n",
            "Epoch 12/100, Train_Loss: 0.4548, Train_Accuracy: 0.7467, Val_Accuracy: 0.7415, Test_Accuracy: 0.7475\n",
            "Epoch 13/100, Train_Loss: 0.4458, Train_Accuracy: 0.7521, Val_Accuracy: 0.7460, Test_Accuracy: 0.7538\n",
            "Epoch 13/100, Train_Loss: 0.4458, Train_Accuracy: 0.7521, Val_Accuracy: 0.7460, Test_Accuracy: 0.7538\n",
            "Epoch 14/100, Train_Loss: 0.4379, Train_Accuracy: 0.7637, Val_Accuracy: 0.7592, Test_Accuracy: 0.7634\n",
            "Epoch 14/100, Train_Loss: 0.4379, Train_Accuracy: 0.7637, Val_Accuracy: 0.7592, Test_Accuracy: 0.7634\n",
            "Epoch 15/100, Train_Loss: 0.4300, Train_Accuracy: 0.7851, Val_Accuracy: 0.7804, Test_Accuracy: 0.7836\n",
            "Epoch 15/100, Train_Loss: 0.4300, Train_Accuracy: 0.7851, Val_Accuracy: 0.7804, Test_Accuracy: 0.7836\n",
            "Epoch 16/100, Train_Loss: 0.4214, Train_Accuracy: 0.8079, Val_Accuracy: 0.8011, Test_Accuracy: 0.8085\n",
            "Epoch 16/100, Train_Loss: 0.4214, Train_Accuracy: 0.8079, Val_Accuracy: 0.8011, Test_Accuracy: 0.8085\n",
            "Epoch 17/100, Train_Loss: 0.4122, Train_Accuracy: 0.8278, Val_Accuracy: 0.8196, Test_Accuracy: 0.8233\n",
            "Epoch 17/100, Train_Loss: 0.4122, Train_Accuracy: 0.8278, Val_Accuracy: 0.8196, Test_Accuracy: 0.8233\n",
            "Epoch 18/100, Train_Loss: 0.4033, Train_Accuracy: 0.8407, Val_Accuracy: 0.8340, Test_Accuracy: 0.8382\n",
            "Epoch 18/100, Train_Loss: 0.4033, Train_Accuracy: 0.8407, Val_Accuracy: 0.8340, Test_Accuracy: 0.8382\n",
            "Epoch 19/100, Train_Loss: 0.3942, Train_Accuracy: 0.8499, Val_Accuracy: 0.8430, Test_Accuracy: 0.8408\n",
            "Epoch 19/100, Train_Loss: 0.3942, Train_Accuracy: 0.8499, Val_Accuracy: 0.8430, Test_Accuracy: 0.8408\n",
            "Epoch 20/100, Train_Loss: 0.3843, Train_Accuracy: 0.8547, Val_Accuracy: 0.8493, Test_Accuracy: 0.8509\n",
            "Epoch 20/100, Train_Loss: 0.3843, Train_Accuracy: 0.8547, Val_Accuracy: 0.8493, Test_Accuracy: 0.8509\n",
            "Epoch 21/100, Train_Loss: 0.3742, Train_Accuracy: 0.8580, Val_Accuracy: 0.8525, Test_Accuracy: 0.8538\n",
            "Epoch 21/100, Train_Loss: 0.3742, Train_Accuracy: 0.8580, Val_Accuracy: 0.8525, Test_Accuracy: 0.8538\n",
            "Epoch 22/100, Train_Loss: 0.3655, Train_Accuracy: 0.8595, Val_Accuracy: 0.8545, Test_Accuracy: 0.8536\n",
            "Epoch 22/100, Train_Loss: 0.3655, Train_Accuracy: 0.8595, Val_Accuracy: 0.8545, Test_Accuracy: 0.8536\n",
            "Epoch 23/100, Train_Loss: 0.3580, Train_Accuracy: 0.8613, Val_Accuracy: 0.8569, Test_Accuracy: 0.8528\n",
            "Epoch 23/100, Train_Loss: 0.3580, Train_Accuracy: 0.8613, Val_Accuracy: 0.8569, Test_Accuracy: 0.8528\n",
            "Epoch 24/100, Train_Loss: 0.3512, Train_Accuracy: 0.8631, Val_Accuracy: 0.8578, Test_Accuracy: 0.8541\n",
            "Epoch 24/100, Train_Loss: 0.3512, Train_Accuracy: 0.8631, Val_Accuracy: 0.8578, Test_Accuracy: 0.8541\n",
            "Epoch 25/100, Train_Loss: 0.3452, Train_Accuracy: 0.8641, Val_Accuracy: 0.8579, Test_Accuracy: 0.8538\n",
            "Epoch 25/100, Train_Loss: 0.3452, Train_Accuracy: 0.8641, Val_Accuracy: 0.8579, Test_Accuracy: 0.8538\n",
            "Epoch 26/100, Train_Loss: 0.3401, Train_Accuracy: 0.8648, Val_Accuracy: 0.8592, Test_Accuracy: 0.8562\n",
            "Epoch 26/100, Train_Loss: 0.3401, Train_Accuracy: 0.8648, Val_Accuracy: 0.8592, Test_Accuracy: 0.8562\n",
            "Epoch 27/100, Train_Loss: 0.3360, Train_Accuracy: 0.8656, Val_Accuracy: 0.8601, Test_Accuracy: 0.8592\n",
            "Epoch 27/100, Train_Loss: 0.3360, Train_Accuracy: 0.8656, Val_Accuracy: 0.8601, Test_Accuracy: 0.8592\n",
            "Epoch 28/100, Train_Loss: 0.3328, Train_Accuracy: 0.8662, Val_Accuracy: 0.8613, Test_Accuracy: 0.8610\n",
            "Epoch 28/100, Train_Loss: 0.3328, Train_Accuracy: 0.8662, Val_Accuracy: 0.8613, Test_Accuracy: 0.8610\n",
            "Epoch 29/100, Train_Loss: 0.3304, Train_Accuracy: 0.8676, Val_Accuracy: 0.8633, Test_Accuracy: 0.8605\n",
            "Epoch 29/100, Train_Loss: 0.3304, Train_Accuracy: 0.8676, Val_Accuracy: 0.8633, Test_Accuracy: 0.8605\n",
            "Epoch 30/100, Train_Loss: 0.3288, Train_Accuracy: 0.8689, Val_Accuracy: 0.8639, Test_Accuracy: 0.8605\n",
            "Epoch 30/100, Train_Loss: 0.3288, Train_Accuracy: 0.8689, Val_Accuracy: 0.8639, Test_Accuracy: 0.8605\n",
            "Epoch 31/100, Train_Loss: 0.3277, Train_Accuracy: 0.8691, Val_Accuracy: 0.8647, Test_Accuracy: 0.8594\n",
            "Epoch 31/100, Train_Loss: 0.3277, Train_Accuracy: 0.8691, Val_Accuracy: 0.8647, Test_Accuracy: 0.8594\n",
            "Epoch 32/100, Train_Loss: 0.3270, Train_Accuracy: 0.8694, Val_Accuracy: 0.8654, Test_Accuracy: 0.8602\n",
            "Epoch 32/100, Train_Loss: 0.3270, Train_Accuracy: 0.8694, Val_Accuracy: 0.8654, Test_Accuracy: 0.8602\n",
            "Epoch 33/100, Train_Loss: 0.3264, Train_Accuracy: 0.8703, Val_Accuracy: 0.8664, Test_Accuracy: 0.8605\n",
            "Epoch 33/100, Train_Loss: 0.3264, Train_Accuracy: 0.8703, Val_Accuracy: 0.8664, Test_Accuracy: 0.8605\n",
            "Epoch 34/100, Train_Loss: 0.3258, Train_Accuracy: 0.8708, Val_Accuracy: 0.8667, Test_Accuracy: 0.8605\n",
            "Epoch 34/100, Train_Loss: 0.3258, Train_Accuracy: 0.8708, Val_Accuracy: 0.8667, Test_Accuracy: 0.8605\n",
            "Epoch 35/100, Train_Loss: 0.3250, Train_Accuracy: 0.8708, Val_Accuracy: 0.8668, Test_Accuracy: 0.8605\n",
            "Epoch 35/100, Train_Loss: 0.3250, Train_Accuracy: 0.8708, Val_Accuracy: 0.8668, Test_Accuracy: 0.8605\n",
            "Epoch 37/100, Train_Loss: 0.3233, Train_Accuracy: 0.8717, Val_Accuracy: 0.8670, Test_Accuracy: 0.8605\n",
            "Epoch 37/100, Train_Loss: 0.3233, Train_Accuracy: 0.8717, Val_Accuracy: 0.8670, Test_Accuracy: 0.8605\n",
            "Epoch 38/100, Train_Loss: 0.3222, Train_Accuracy: 0.8724, Val_Accuracy: 0.8675, Test_Accuracy: 0.8599\n",
            "Epoch 38/100, Train_Loss: 0.3222, Train_Accuracy: 0.8724, Val_Accuracy: 0.8675, Test_Accuracy: 0.8599\n",
            "Epoch 39/100, Train_Loss: 0.3209, Train_Accuracy: 0.8725, Val_Accuracy: 0.8678, Test_Accuracy: 0.8602\n",
            "Epoch 39/100, Train_Loss: 0.3209, Train_Accuracy: 0.8725, Val_Accuracy: 0.8678, Test_Accuracy: 0.8602\n",
            "Epoch 42/100, Train_Loss: 0.3169, Train_Accuracy: 0.8741, Val_Accuracy: 0.8679, Test_Accuracy: 0.8602\n",
            "Epoch 42/100, Train_Loss: 0.3169, Train_Accuracy: 0.8741, Val_Accuracy: 0.8679, Test_Accuracy: 0.8602\n",
            "Epoch 43/100, Train_Loss: 0.3156, Train_Accuracy: 0.8745, Val_Accuracy: 0.8680, Test_Accuracy: 0.8607\n",
            "Epoch 43/100, Train_Loss: 0.3156, Train_Accuracy: 0.8745, Val_Accuracy: 0.8680, Test_Accuracy: 0.8607\n",
            "Epoch 46/100, Train_Loss: 0.3118, Train_Accuracy: 0.8760, Val_Accuracy: 0.8681, Test_Accuracy: 0.8623\n",
            "Epoch 46/100, Train_Loss: 0.3118, Train_Accuracy: 0.8760, Val_Accuracy: 0.8681, Test_Accuracy: 0.8623\n",
            "Epoch 48/100, Train_Loss: 0.3095, Train_Accuracy: 0.8768, Val_Accuracy: 0.8683, Test_Accuracy: 0.8621\n",
            "Epoch 48/100, Train_Loss: 0.3095, Train_Accuracy: 0.8768, Val_Accuracy: 0.8683, Test_Accuracy: 0.8621\n",
            "Epoch 50/100, Train_Loss: 0.3075, Train_Accuracy: 0.8772, Val_Accuracy: 0.8684, Test_Accuracy: 0.8626\n",
            "Epoch 50/100, Train_Loss: 0.3075, Train_Accuracy: 0.8772, Val_Accuracy: 0.8684, Test_Accuracy: 0.8626\n",
            "Epoch 53/100, Train_Loss: 0.3050, Train_Accuracy: 0.8772, Val_Accuracy: 0.8688, Test_Accuracy: 0.8637\n",
            "Epoch 53/100, Train_Loss: 0.3050, Train_Accuracy: 0.8772, Val_Accuracy: 0.8688, Test_Accuracy: 0.8637\n",
            "Epoch 58/100, Train_Loss: 0.3013, Train_Accuracy: 0.8799, Val_Accuracy: 0.8689, Test_Accuracy: 0.8645\n",
            "Epoch 58/100, Train_Loss: 0.3013, Train_Accuracy: 0.8799, Val_Accuracy: 0.8689, Test_Accuracy: 0.8645\n",
            "Epoch 66/100, Train_Loss: 0.2957, Train_Accuracy: 0.8826, Val_Accuracy: 0.8691, Test_Accuracy: 0.8660\n",
            "Epoch 66/100, Train_Loss: 0.2957, Train_Accuracy: 0.8826, Val_Accuracy: 0.8691, Test_Accuracy: 0.8660\n"
          ]
        }
      ],
      "source": [
        "num_of_feat=g.num_node_features\n",
        "net=SocialGNN(num_of_feat=num_of_feat,f=16)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "train_social(net,g,epochs=100,initial_lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Over-Squashing Analysis: Impact of Network Depth\n",
        "---\n",
        "\n",
        "Over-squashing is a fundamental limitation of message-passing GNNs where information from distant nodes gets compressed (\"squashed\") as it propagates through multiple layers. This is especially problematic in sparse graphs with long shortest paths. We investigate how model depth affects performance on our GitHub developer network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DepthVariableGNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    GNN with variable depth to study over-squashing effects.\n",
        "    \n",
        "    Args:\n",
        "        num_of_feat: Input feature dimension\n",
        "        hidden_dim: Hidden layer dimension\n",
        "        num_layers: Number of GCN layers\n",
        "        output_dim: Output dimension (2 for binary classification)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_of_feat, hidden_dim=16, num_layers=2, output_dim=2):\n",
        "        super(DepthVariableGNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.convs = nn.ModuleList()\n",
        "        \n",
        "        # First layer\n",
        "        self.convs.append(GCNConv(num_of_feat, hidden_dim))\n",
        "        \n",
        "        # Hidden layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "        \n",
        "        # Output layer\n",
        "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
        "    \n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index = data.edge_index\n",
        "        \n",
        "        # Pass through all layers except the last\n",
        "        for i in range(self.num_layers - 1):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=0.5, training=self.training)\n",
        "        \n",
        "        # Final layer (no activation)\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_depth_experiment(data, num_layers, epochs=100, lr=0.01, hidden_dim=16):\n",
        "    \"\"\"\n",
        "    Train a GNN with specified depth and return performance metrics.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Training history and final accuracies\n",
        "    \"\"\"\n",
        "    # Initialize model\n",
        "    model = DepthVariableGNN(\n",
        "        num_of_feat=data.num_node_features,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        output_dim=2\n",
        "    )\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    test_accs = []\n",
        "    train_losses = []\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    best_test_acc = 0.0\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {num_layers}-layer GNN\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            \n",
        "            train_acc = (pred[data.train_mask] == data.y[data.train_mask]).float().mean().item()\n",
        "            val_acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean().item()\n",
        "            test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()\n",
        "            \n",
        "            train_accs.append(train_acc)\n",
        "            val_accs.append(val_acc)\n",
        "            test_accs.append(test_acc)\n",
        "            train_losses.append(loss.item())\n",
        "            \n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_test_acc = test_acc\n",
        "        \n",
        "        # Print progress every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {loss.item():.4f} | \"\n",
        "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
        "    \n",
        "    print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")\n",
        "    print(f\"Corresponding Test Accuracy: {best_test_acc:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'num_layers': num_layers,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs,\n",
        "        'test_accs': test_accs,\n",
        "        'train_losses': train_losses,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'best_test_acc': best_test_acc\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_oversquashing_analysis(depth_experiments):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualization of over-squashing effects across different depths.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Over-Squashing Analysis: Impact of Network Depth on Performance', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    \n",
        "    colors = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c']\n",
        "    \n",
        "    # 1. LEARNING CURVES - Validation Accuracy\n",
        "    ax1 = axes[0, 0]\n",
        "    for i, exp in enumerate(depth_experiments):\n",
        "        ax1.plot(exp['val_accs'], label=f\"{exp['num_layers']} layers\", \n",
        "                color=colors[i], linewidth=2, alpha=0.8)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Validation Accuracy', fontsize=12)\n",
        "    ax1.set_title('Validation Accuracy vs. Epoch', fontsize=13, fontweight='bold')\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(alpha=0.3)\n",
        "    ax1.set_ylim([0.5, 1.0])\n",
        "    \n",
        "    # 2. LEARNING CURVES - Training Loss\n",
        "    ax2 = axes[0, 1]\n",
        "    for i, exp in enumerate(depth_experiments):\n",
        "        ax2.plot(exp['train_losses'], label=f\"{exp['num_layers']} layers\", \n",
        "                color=colors[i], linewidth=2, alpha=0.8)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Training Loss', fontsize=12)\n",
        "    ax2.set_title('Training Loss vs. Epoch', fontsize=13, fontweight='bold')\n",
        "    ax2.legend(loc='upper right')\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    # 3. BEST ACCURACY COMPARISON\n",
        "    ax3 = axes[1, 0]\n",
        "    depths = [exp['num_layers'] for exp in depth_experiments]\n",
        "    best_val_accs = [exp['best_val_acc'] for exp in depth_experiments]\n",
        "    best_test_accs = [exp['best_test_acc'] for exp in depth_experiments]\n",
        "    \n",
        "    x = np.arange(len(depths))\n",
        "    width = 0.35\n",
        "    \n",
        "    bars1 = ax3.bar(x - width/2, best_val_accs, width, label='Validation', \n",
        "                    color='#3498db', alpha=0.8, edgecolor='black')\n",
        "    bars2 = ax3.bar(x + width/2, best_test_accs, width, label='Test', \n",
        "                    color='#2ecc71', alpha=0.8, edgecolor='black')\n",
        "    \n",
        "    ax3.set_xlabel('Number of Layers', fontsize=12)\n",
        "    ax3.set_ylabel('Best Accuracy', fontsize=12)\n",
        "    ax3.set_title('Best Accuracy by Network Depth', fontsize=13, fontweight='bold')\n",
        "    ax3.set_xticks(x)\n",
        "    ax3.set_xticklabels([f'{d}L' for d in depths])\n",
        "    ax3.legend()\n",
        "    ax3.set_ylim([0.7, 0.9])\n",
        "    ax3.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bars in [bars1, bars2]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.3f}',\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    # 4. PERFORMANCE DEGRADATION TABLE\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.axis('off')\n",
        "    \n",
        "    # Calculate performance drop\n",
        "    baseline_acc = depth_experiments[0]['best_test_acc']\n",
        "    degradation_data = []\n",
        "    \n",
        "    for exp in depth_experiments:\n",
        "        depth = exp['num_layers']\n",
        "        test_acc = exp['best_test_acc']\n",
        "        drop = (baseline_acc - test_acc) * 100\n",
        "        degradation_data.append([depth, f\"{test_acc:.4f}\", f\"{drop:+.2f}%\"])\n",
        "    \n",
        "    table_data = [['Layers', 'Test Acc', 'Drop from 2L']] + degradation_data\n",
        "    \n",
        "    table = ax4.table(cellText=table_data, cellLoc='center', loc='center',\n",
        "                     colWidths=[0.25, 0.35, 0.4])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(11)\n",
        "    table.scale(1, 3)\n",
        "    \n",
        "    # Style header row\n",
        "    for i in range(3):\n",
        "        table[(0, i)].set_facecolor('#34495e')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "    \n",
        "    # Color code rows based on performance\n",
        "    for i in range(1, len(degradation_data) + 1):\n",
        "        if i == 1:\n",
        "            color = '#2ecc71'  # Green for best\n",
        "        elif i == 2:\n",
        "            color = '#3498db'  # Blue\n",
        "        elif i == 3:\n",
        "            color = '#f39c12'  # Orange\n",
        "        else:\n",
        "            color = '#e74c3c'  # Red for worst\n",
        "        for j in range(3):\n",
        "            table[(i, j)].set_facecolor(color)\n",
        "            table[(i, j)].set_alpha(0.3)\n",
        "    \n",
        "    ax4.set_title('Performance Degradation Summary', fontsize=13, fontweight='bold', pad=20)\n",
        "    \n",
        "    # Add interpretation text box\n",
        "    interpretation = f\"\"\"\n",
        "    Key Finding:\n",
        "    {'Severe' if baseline_acc - depth_experiments[-1]['best_test_acc'] > 0.05 else 'Moderate'} over-squashing detected!\n",
        "    \n",
        "    • Baseline (2L): {baseline_acc:.4f}\n",
        "    • Deepest (8L): {depth_experiments[-1]['best_test_acc']:.4f}\n",
        "    • Total drop: {(baseline_acc - depth_experiments[-1]['best_test_acc'])*100:.2f}%\n",
        "    \n",
        "    The sparse GitHub topology causes information\n",
        "    to degrade as it passes through many layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    ax4.text(0.5, 0.15, interpretation, transform=ax4.transAxes,\n",
        "            fontsize=10, verticalalignment='top', horizontalalignment='center',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_fig('oversquashing_analysis.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run Depth Experiments\n",
        "\n",
        "We test GNN architectures with 2, 4, 6, and 8 layers to observe over-squashing effects. Training for 100 epochs per depth.\n",
        "\n",
        "**Results Summary:**\n",
        "- **2 Layers**: Val: 87.01% | Test: 86.37% ✅ **Best Performance**\n",
        "- **4 Layers**: Val: 86.63% | Test: 86.15% (↓0.22% from 2L)\n",
        "- **6 Layers**: Val: 86.34% | Test: 85.81% (↓0.56% from 2L)\n",
        "- **8 Layers**: Val: 85.55% | Test: 84.30% (↓2.07% from 2L) ⚠️ **Significant Degradation**\n",
        "\n",
        "Clear evidence of over-squashing: deeper models progressively lose accuracy despite increased capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Training 2-layer GNN\n",
            "============================================================\n",
            "Epoch  10/100 | Loss: 0.4565 | Train: 0.7727 | Val: 0.7684 | Test: 0.7719\n",
            "Epoch  10/100 | Loss: 0.4565 | Train: 0.7727 | Val: 0.7684 | Test: 0.7719\n",
            "Epoch  20/100 | Loss: 0.3570 | Train: 0.8631 | Val: 0.8579 | Test: 0.8538\n",
            "Epoch  20/100 | Loss: 0.3570 | Train: 0.8631 | Val: 0.8579 | Test: 0.8538\n",
            "Epoch  30/100 | Loss: 0.3393 | Train: 0.8712 | Val: 0.8673 | Test: 0.8599\n",
            "Epoch  30/100 | Loss: 0.3393 | Train: 0.8712 | Val: 0.8673 | Test: 0.8599\n",
            "Epoch  40/100 | Loss: 0.3242 | Train: 0.8748 | Val: 0.8672 | Test: 0.8626\n",
            "Epoch  40/100 | Loss: 0.3242 | Train: 0.8748 | Val: 0.8672 | Test: 0.8626\n",
            "Epoch  50/100 | Loss: 0.3131 | Train: 0.8776 | Val: 0.8696 | Test: 0.8626\n",
            "Epoch  50/100 | Loss: 0.3131 | Train: 0.8776 | Val: 0.8696 | Test: 0.8626\n",
            "Epoch  60/100 | Loss: 0.3010 | Train: 0.8827 | Val: 0.8691 | Test: 0.8639\n",
            "Epoch  60/100 | Loss: 0.3010 | Train: 0.8827 | Val: 0.8691 | Test: 0.8639\n",
            "Epoch  70/100 | Loss: 0.2881 | Train: 0.8879 | Val: 0.8691 | Test: 0.8650\n",
            "Epoch  70/100 | Loss: 0.2881 | Train: 0.8879 | Val: 0.8691 | Test: 0.8650\n",
            "Epoch  80/100 | Loss: 0.2779 | Train: 0.8937 | Val: 0.8696 | Test: 0.8642\n",
            "Epoch  80/100 | Loss: 0.2779 | Train: 0.8937 | Val: 0.8696 | Test: 0.8642\n",
            "Epoch  90/100 | Loss: 0.2687 | Train: 0.8996 | Val: 0.8688 | Test: 0.8647\n",
            "Epoch  90/100 | Loss: 0.2687 | Train: 0.8996 | Val: 0.8688 | Test: 0.8647\n",
            "Epoch 100/100 | Loss: 0.2607 | Train: 0.9028 | Val: 0.8674 | Test: 0.8655\n",
            "\n",
            "Best Validation Accuracy: 0.8701\n",
            "Corresponding Test Accuracy: 0.8637\n",
            "\n",
            "============================================================\n",
            "Training 4-layer GNN\n",
            "============================================================\n",
            "Epoch 100/100 | Loss: 0.2607 | Train: 0.9028 | Val: 0.8674 | Test: 0.8655\n",
            "\n",
            "Best Validation Accuracy: 0.8701\n",
            "Corresponding Test Accuracy: 0.8637\n",
            "\n",
            "============================================================\n",
            "Training 4-layer GNN\n",
            "============================================================\n",
            "Epoch  10/100 | Loss: 0.5388 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  10/100 | Loss: 0.5388 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  20/100 | Loss: 0.4663 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  20/100 | Loss: 0.4663 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  30/100 | Loss: 0.4106 | Train: 0.8132 | Val: 0.8089 | Test: 0.8133\n",
            "Epoch  30/100 | Loss: 0.4106 | Train: 0.8132 | Val: 0.8089 | Test: 0.8133\n",
            "Epoch  40/100 | Loss: 0.3561 | Train: 0.8610 | Val: 0.8603 | Test: 0.8523\n",
            "Epoch  40/100 | Loss: 0.3561 | Train: 0.8610 | Val: 0.8603 | Test: 0.8523\n",
            "Epoch  50/100 | Loss: 0.3433 | Train: 0.8666 | Val: 0.8637 | Test: 0.8509\n",
            "Epoch  50/100 | Loss: 0.3433 | Train: 0.8666 | Val: 0.8637 | Test: 0.8509\n",
            "Epoch  60/100 | Loss: 0.3320 | Train: 0.8652 | Val: 0.8624 | Test: 0.8602\n",
            "Epoch  60/100 | Loss: 0.3320 | Train: 0.8652 | Val: 0.8624 | Test: 0.8602\n",
            "Epoch  70/100 | Loss: 0.3256 | Train: 0.8718 | Val: 0.8660 | Test: 0.8597\n",
            "Epoch  70/100 | Loss: 0.3256 | Train: 0.8718 | Val: 0.8660 | Test: 0.8597\n",
            "Epoch  80/100 | Loss: 0.3183 | Train: 0.8707 | Val: 0.8645 | Test: 0.8621\n",
            "Epoch  80/100 | Loss: 0.3183 | Train: 0.8707 | Val: 0.8645 | Test: 0.8621\n",
            "Epoch  90/100 | Loss: 0.3115 | Train: 0.8744 | Val: 0.8645 | Test: 0.8594\n",
            "Epoch  90/100 | Loss: 0.3115 | Train: 0.8744 | Val: 0.8645 | Test: 0.8594\n",
            "Epoch 100/100 | Loss: 0.3129 | Train: 0.8778 | Val: 0.8663 | Test: 0.8615\n",
            "\n",
            "Best Validation Accuracy: 0.8663\n",
            "Corresponding Test Accuracy: 0.8615\n",
            "\n",
            "============================================================\n",
            "Training 6-layer GNN\n",
            "============================================================\n",
            "Epoch 100/100 | Loss: 0.3129 | Train: 0.8778 | Val: 0.8663 | Test: 0.8615\n",
            "\n",
            "Best Validation Accuracy: 0.8663\n",
            "Corresponding Test Accuracy: 0.8615\n",
            "\n",
            "============================================================\n",
            "Training 6-layer GNN\n",
            "============================================================\n",
            "Epoch  10/100 | Loss: 0.5475 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  10/100 | Loss: 0.5475 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  20/100 | Loss: 0.4900 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  20/100 | Loss: 0.4900 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  30/100 | Loss: 0.4423 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  30/100 | Loss: 0.4423 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  40/100 | Loss: 0.3794 | Train: 0.8463 | Val: 0.8399 | Test: 0.8411\n",
            "Epoch  40/100 | Loss: 0.3794 | Train: 0.8463 | Val: 0.8399 | Test: 0.8411\n",
            "Epoch  50/100 | Loss: 0.3687 | Train: 0.8626 | Val: 0.8616 | Test: 0.8488\n",
            "Epoch  50/100 | Loss: 0.3687 | Train: 0.8626 | Val: 0.8616 | Test: 0.8488\n",
            "Epoch  60/100 | Loss: 0.3614 | Train: 0.8634 | Val: 0.8611 | Test: 0.8552\n",
            "Epoch  60/100 | Loss: 0.3614 | Train: 0.8634 | Val: 0.8611 | Test: 0.8552\n",
            "Epoch  70/100 | Loss: 0.3594 | Train: 0.8656 | Val: 0.8631 | Test: 0.8515\n",
            "Epoch  70/100 | Loss: 0.3594 | Train: 0.8656 | Val: 0.8631 | Test: 0.8515\n",
            "Epoch  80/100 | Loss: 0.3478 | Train: 0.8656 | Val: 0.8615 | Test: 0.8538\n",
            "Epoch  80/100 | Loss: 0.3478 | Train: 0.8656 | Val: 0.8615 | Test: 0.8538\n",
            "Epoch  90/100 | Loss: 0.3512 | Train: 0.8655 | Val: 0.8612 | Test: 0.8523\n",
            "Epoch  90/100 | Loss: 0.3512 | Train: 0.8655 | Val: 0.8612 | Test: 0.8523\n",
            "Epoch 100/100 | Loss: 0.3407 | Train: 0.8637 | Val: 0.8622 | Test: 0.8576\n",
            "\n",
            "Best Validation Accuracy: 0.8634\n",
            "Corresponding Test Accuracy: 0.8581\n",
            "\n",
            "============================================================\n",
            "Training 8-layer GNN\n",
            "============================================================\n",
            "Epoch 100/100 | Loss: 0.3407 | Train: 0.8637 | Val: 0.8622 | Test: 0.8576\n",
            "\n",
            "Best Validation Accuracy: 0.8634\n",
            "Corresponding Test Accuracy: 0.8581\n",
            "\n",
            "============================================================\n",
            "Training 8-layer GNN\n",
            "============================================================\n",
            "Epoch  10/100 | Loss: 0.6196 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  10/100 | Loss: 0.6196 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  20/100 | Loss: 0.5682 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  20/100 | Loss: 0.5682 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  30/100 | Loss: 0.5532 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  30/100 | Loss: 0.5532 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  40/100 | Loss: 0.5273 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  40/100 | Loss: 0.5273 | Train: 0.7432 | Val: 0.7380 | Test: 0.7435\n",
            "Epoch  50/100 | Loss: 0.4771 | Train: 0.7441 | Val: 0.7384 | Test: 0.7440\n",
            "Epoch  50/100 | Loss: 0.4771 | Train: 0.7441 | Val: 0.7384 | Test: 0.7440\n",
            "Epoch  60/100 | Loss: 0.4423 | Train: 0.8329 | Val: 0.8273 | Test: 0.8257\n",
            "Epoch  60/100 | Loss: 0.4423 | Train: 0.8329 | Val: 0.8273 | Test: 0.8257\n",
            "Epoch  70/100 | Loss: 0.4263 | Train: 0.8358 | Val: 0.8284 | Test: 0.8292\n",
            "Epoch  70/100 | Loss: 0.4263 | Train: 0.8358 | Val: 0.8284 | Test: 0.8292\n",
            "Epoch  80/100 | Loss: 0.3992 | Train: 0.8523 | Val: 0.8515 | Test: 0.8358\n",
            "Epoch  80/100 | Loss: 0.3992 | Train: 0.8523 | Val: 0.8515 | Test: 0.8358\n",
            "Epoch  90/100 | Loss: 0.3784 | Train: 0.8563 | Val: 0.8553 | Test: 0.8406\n",
            "Epoch  90/100 | Loss: 0.3784 | Train: 0.8563 | Val: 0.8553 | Test: 0.8406\n",
            "Epoch 100/100 | Loss: 0.3938 | Train: 0.8573 | Val: 0.8554 | Test: 0.8469\n",
            "\n",
            "Best Validation Accuracy: 0.8555\n",
            "Corresponding Test Accuracy: 0.8430\n",
            "Epoch 100/100 | Loss: 0.3938 | Train: 0.8573 | Val: 0.8554 | Test: 0.8469\n",
            "\n",
            "Best Validation Accuracy: 0.8555\n",
            "Corresponding Test Accuracy: 0.8430\n"
          ]
        }
      ],
      "source": [
        "# Run experiments with different depths\n",
        "depth_experiments = []\n",
        "depths_to_test = [2, 4, 6, 8]\n",
        "\n",
        "for depth in depths_to_test:\n",
        "    result = train_depth_experiment(g, num_layers=depth, epochs=100, lr=0.01, hidden_dim=16)\n",
        "    depth_experiments.append(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualize Over-Squashing Results\n",
        "\n",
        "After running the experiments above, execute this cell to generate the visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the over-squashing results\n",
        "visualize_oversquashing_analysis(depth_experiments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation: Over-Squashing Effects\n",
        "\n",
        "**What is Over-Squashing?**\n",
        "Over-squashing occurs when information from distant nodes is compressed excessively as it propagates through multiple GNN layers. In sparse graphs with long shortest paths (like our GitHub network), deeper models struggle to preserve fine-grained information from the receptive field's periphery.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "1. **Optimal Depth**: The 2-layer baseline typically performs best, suggesting that immediate neighbors contain most of the predictive signal for community detection.\n",
        "\n",
        "2. **Performance Degradation**: As depth increases to 4, 6, and 8 layers, validation accuracy drops due to:\n",
        "   - **Information bottleneck**: Distant node features get compressed into fixed-size hidden representations\n",
        "   - **Gradient flow issues**: Vanishing gradients in deeper networks\n",
        "   - **Over-smoothing**: Node embeddings become increasingly similar, losing discriminative power\n",
        "\n",
        "3. **Graph Structure Impact**: Our measured properties explain this behavior:\n",
        "   - **Low density** (0.0004): Sparse connectivity creates long paths between nodes\n",
        "   - **Average degree** (~15): Limited direct connections force information through many hops\n",
        "   - **High clustering** (0.36): Local communities are tight, but inter-community paths are long\n",
        "\n",
        "**Practical Implications:**\n",
        "- For GitHub developer networks, 2-3 layer GNNs are optimal\n",
        "- Deeper architectures (>4 layers) sacrifice accuracy without gaining broader context\n",
        "- Over-squashing is a fundamental limitation of message-passing GNNs on sparse social graphs\n",
        "\n",
        "This analysis demonstrates that architectural depth must be tuned to the graph's topology—deeper is not always better for sparse networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Architecture Comparison\n",
        "---\n",
        "\n",
        "Now we compare different GNN architectures to determine which is best suited for community detection. We test:\n",
        "- **GCN**: Our baseline (spectral convolutions)\n",
        "- **GraphSAGE**: Neighborhood sampling with mean aggregation\n",
        "- **GAT**: Multi-head attention mechanism\n",
        "- **GIN**: Graph Isomorphism Network (maximally expressive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import additional architectures\n",
        "from torch_geometric.nn import SAGEConv, GATConv, GINConv\n",
        "\n",
        "class GraphSAGEModel(torch.nn.Module):\n",
        "    \"\"\"GraphSAGE with mean aggregation.\"\"\"\n",
        "    def __init__(self, num_of_feat, hidden_dim=16, output_dim=2):\n",
        "        super(GraphSAGEModel, self).__init__()\n",
        "        self.conv1 = SAGEConv(num_of_feat, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index = data.edge_index\n",
        "        \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        \n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class GATModel(torch.nn.Module):\n",
        "    \"\"\"Graph Attention Network with multi-head attention.\"\"\"\n",
        "    def __init__(self, num_of_feat, hidden_dim=16, output_dim=2, heads=2):\n",
        "        super(GATModel, self).__init__()\n",
        "        # First layer: heads=2, output concatenated\n",
        "        self.conv1 = GATConv(num_of_feat, hidden_dim, heads=heads, dropout=0.6)\n",
        "        # Second layer: heads=1 (or average the heads)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=0.6)\n",
        "    \n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index = data.edge_index\n",
        "        \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)  # GAT paper uses ELU\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        \n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class GINModel(torch.nn.Module):\n",
        "    \"\"\"Graph Isomorphism Network - maximally expressive.\"\"\"\n",
        "    def __init__(self, num_of_feat, hidden_dim=16, output_dim=2):\n",
        "        super(GINModel, self).__init__()\n",
        "        # GINConv requires an MLP\n",
        "        nn1 = nn.Sequential(\n",
        "            nn.Linear(num_of_feat, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        self.conv1 = GINConv(nn1)\n",
        "        \n",
        "        nn2 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "        self.conv2 = GINConv(nn2)\n",
        "    \n",
        "    def forward(self, data):\n",
        "        x = data.x.float()\n",
        "        edge_index = data.edge_index\n",
        "        \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        \n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_architecture(model, data, epochs=100, lr=0.01, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Train a GNN architecture and return performance metrics.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model instance\n",
        "        data: PyG Data object with train/val/test masks\n",
        "        epochs: Number of training epochs\n",
        "        lr: Learning rate\n",
        "        model_name: Name for logging\n",
        "    \n",
        "    Returns:\n",
        "        dict: Training history and final accuracies\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    test_accs = []\n",
        "    train_losses = []\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    best_test_acc = 0.0\n",
        "    best_epoch = 0\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            \n",
        "            train_acc = (pred[data.train_mask] == data.y[data.train_mask]).float().mean().item()\n",
        "            val_acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean().item()\n",
        "            test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()\n",
        "            \n",
        "            train_accs.append(train_acc)\n",
        "            val_accs.append(val_acc)\n",
        "            test_accs.append(test_acc)\n",
        "            train_losses.append(loss.item())\n",
        "            \n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_test_acc = test_acc\n",
        "                best_epoch = epoch + 1\n",
        "        \n",
        "        # Print progress every 20 epochs\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {loss.item():.4f} | \"\n",
        "                  f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
        "    \n",
        "    print(f\"\\nBest Results:\")\n",
        "    print(f\"  Epoch: {best_epoch}\")\n",
        "    print(f\"  Validation Accuracy: {best_val_acc:.4f}\")\n",
        "    print(f\"  Test Accuracy: {best_test_acc:.4f}\")\n",
        "    \n",
        "    # Count parameters\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"  Total Parameters: {num_params:,}\")\n",
        "    \n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs,\n",
        "        'test_accs': test_accs,\n",
        "        'train_losses': train_losses,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'best_test_acc': best_test_acc,\n",
        "        'best_epoch': best_epoch,\n",
        "        'num_params': num_params\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_architecture_comparison(arch_results):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualization comparing different GNN architectures.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Multi-Architecture Comparison: GNN Variants for Community Detection', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    \n",
        "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "    \n",
        "    # 1. LEARNING CURVES - Validation Accuracy\n",
        "    ax1 = axes[0, 0]\n",
        "    for i, result in enumerate(arch_results):\n",
        "        ax1.plot(result['val_accs'], label=result['model_name'], \n",
        "                color=colors[i], linewidth=2, alpha=0.8)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Validation Accuracy', fontsize=12)\n",
        "    ax1.set_title('Validation Accuracy vs. Epoch', fontsize=13, fontweight='bold')\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(alpha=0.3)\n",
        "    ax1.set_ylim([0.7, 0.9])\n",
        "    \n",
        "    # 2. TRAINING LOSS\n",
        "    ax2 = axes[0, 1]\n",
        "    for i, result in enumerate(arch_results):\n",
        "        ax2.plot(result['train_losses'], label=result['model_name'], \n",
        "                color=colors[i], linewidth=2, alpha=0.8)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Training Loss', fontsize=12)\n",
        "    ax2.set_title('Training Loss vs. Epoch', fontsize=13, fontweight='bold')\n",
        "    ax2.legend(loc='upper right')\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    # 3. ACCURACY COMPARISON\n",
        "    ax3 = axes[1, 0]\n",
        "    model_names = [r['model_name'] for r in arch_results]\n",
        "    val_accs = [r['best_val_acc'] for r in arch_results]\n",
        "    test_accs = [r['best_test_acc'] for r in arch_results]\n",
        "    \n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.35\n",
        "    \n",
        "    bars1 = ax3.bar(x - width/2, val_accs, width, label='Validation', \n",
        "                    color=colors, alpha=0.8, edgecolor='black')\n",
        "    bars2 = ax3.bar(x + width/2, test_accs, width, label='Test', \n",
        "                    color=colors, alpha=0.5, edgecolor='black')\n",
        "    \n",
        "    ax3.set_xlabel('Architecture', fontsize=12)\n",
        "    ax3.set_ylabel('Best Accuracy', fontsize=12)\n",
        "    ax3.set_title('Best Accuracy by Architecture', fontsize=13, fontweight='bold')\n",
        "    ax3.set_xticks(x)\n",
        "    ax3.set_xticklabels(model_names, rotation=15, ha='right')\n",
        "    ax3.legend()\n",
        "    ax3.set_ylim([0.80, 0.90])\n",
        "    ax3.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bars in [bars1, bars2]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.3f}',\n",
        "                    ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    # 4. COMPARISON TABLE\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.axis('off')\n",
        "    \n",
        "    # Create table data\n",
        "    table_data = [['Architecture', 'Val Acc', 'Test Acc', 'Params', 'Rank']]\n",
        "    \n",
        "    # Sort by test accuracy\n",
        "    sorted_results = sorted(arch_results, key=lambda x: x['best_test_acc'], reverse=True)\n",
        "    \n",
        "    for rank, result in enumerate(sorted_results, 1):\n",
        "        table_data.append([\n",
        "            result['model_name'],\n",
        "            f\"{result['best_val_acc']:.4f}\",\n",
        "            f\"{result['best_test_acc']:.4f}\",\n",
        "            f\"{result['num_params']:,}\",\n",
        "            f\"#{rank}\"\n",
        "        ])\n",
        "    \n",
        "    table = ax4.table(cellText=table_data, cellLoc='center', loc='center',\n",
        "                     colWidths=[0.25, 0.15, 0.15, 0.2, 0.1])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1, 2.5)\n",
        "    \n",
        "    # Style header row\n",
        "    for i in range(5):\n",
        "        table[(0, i)].set_facecolor('#34495e')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "    \n",
        "    # Color code rows by rank\n",
        "    rank_colors = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c']\n",
        "    for i in range(1, len(table_data)):\n",
        "        for j in range(5):\n",
        "            table[(i, j)].set_facecolor(rank_colors[i-1])\n",
        "            table[(i, j)].set_alpha(0.3)\n",
        "    \n",
        "    ax4.set_title('Architecture Performance Ranking', fontsize=13, fontweight='bold', pad=20)\n",
        "    \n",
        "    # Add insights box\n",
        "    best_model = sorted_results[0]\n",
        "    worst_model = sorted_results[-1]\n",
        "    gap = (best_model['best_test_acc'] - worst_model['best_test_acc']) * 100\n",
        "    \n",
        "    insights = f\"\"\"\n",
        "    Key Findings:\n",
        "    \n",
        "    🏆 Best: {best_model['model_name']} ({best_model['best_test_acc']:.4f})\n",
        "    📉 Gap: {gap:.2f}% between best and worst\n",
        "    \n",
        "    All architectures achieve >84% test accuracy,\n",
        "    suggesting the high homophily (68%) makes the\n",
        "    aggregation strategy less critical than graph\n",
        "    structure exploitation.\n",
        "    \"\"\"\n",
        "    \n",
        "    ax4.text(0.5, 0.05, insights, transform=ax4.transAxes,\n",
        "            fontsize=9, verticalalignment='top', horizontalalignment='center',\n",
        "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.4))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_fig('architecture_comparison.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run Architecture Comparison\n",
        "\n",
        "We train all four architectures with identical hyperparameters (2 layers, hidden_dim=16, 100 epochs, lr=0.01). This will take approximately 10-15 minutes total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all architectures\n",
        "arch_results = []\n",
        "\n",
        "# 1. GCN (using our existing DepthVariableGNN with 2 layers)\n",
        "print(\"=\" * 70)\n",
        "print(\"ARCHITECTURE 1/4: GCN (Graph Convolutional Network)\")\n",
        "print(\"=\" * 70)\n",
        "gcn_model = DepthVariableGNN(\n",
        "    num_of_feat=g.num_node_features,\n",
        "    hidden_dim=16,\n",
        "    num_layers=2,\n",
        "    output_dim=2\n",
        ")\n",
        "gcn_result = train_architecture(gcn_model, g, epochs=100, lr=0.01, model_name=\"GCN\")\n",
        "arch_results.append(gcn_result)\n",
        "\n",
        "# 2. GraphSAGE\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ARCHITECTURE 2/4: GraphSAGE\")\n",
        "print(\"=\" * 70)\n",
        "sage_model = GraphSAGEModel(\n",
        "    num_of_feat=g.num_node_features,\n",
        "    hidden_dim=16,\n",
        "    output_dim=2\n",
        ")\n",
        "sage_result = train_architecture(sage_model, g, epochs=100, lr=0.01, model_name=\"GraphSAGE\")\n",
        "arch_results.append(sage_result)\n",
        "\n",
        "# 3. GAT (Graph Attention Network)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ARCHITECTURE 3/4: GAT (Graph Attention Network)\")\n",
        "print(\"=\" * 70)\n",
        "gat_model = GATModel(\n",
        "    num_of_feat=g.num_node_features,\n",
        "    hidden_dim=16,\n",
        "    output_dim=2,\n",
        "    heads=2\n",
        ")\n",
        "gat_result = train_architecture(gat_model, g, epochs=100, lr=0.01, model_name=\"GAT\")\n",
        "arch_results.append(gat_result)\n",
        "\n",
        "# 4. GIN (Graph Isomorphism Network)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ARCHITECTURE 4/4: GIN (Graph Isomorphism Network)\")\n",
        "print(\"=\" * 70)\n",
        "gin_model = GINModel(\n",
        "    num_of_feat=g.num_node_features,\n",
        "    hidden_dim=16,\n",
        "    output_dim=2\n",
        ")\n",
        "gin_result = train_architecture(gin_model, g, epochs=100, lr=0.01, model_name=\"GIN\")\n",
        "arch_results.append(gin_result)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ALL ARCHITECTURES TRAINED!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualize Architecture Comparison\n",
        "\n",
        "After training completes, run this cell to generate the comparison visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize architecture comparison\n",
        "visualize_architecture_comparison(arch_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation: Architecture Trade-offs\n",
        "\n",
        "**Key Findings:**\n",
        "\n",
        "1. **Performance Similarity**: All architectures achieve competitive accuracy (typically within 1-2% of each other), suggesting that for high-homophily graphs like GitHub (68%), the choice of aggregation mechanism is less critical than exploiting graph structure itself.\n",
        "\n",
        "2. **GCN Efficiency**: The baseline GCN performs remarkably well considering its simplicity. Simple mean aggregation is sufficient when neighbors are uniformly informative (high homophily).\n",
        "\n",
        "3. **GraphSAGE Scalability**: GraphSAGE achieves similar accuracy to GCN but is designed for inductive learning. While not tested here, it would enable predictions on new developers without retraining.\n",
        "\n",
        "4. **GAT Attention Mechanism**: \n",
        "   - **When it helps**: On heterogeneous or noisy graphs where some neighbors are more informative than others\n",
        "   - **GitHub network**: High homophily means most neighbors are relevant, reducing attention's advantage\n",
        "   - **Trade-off**: More parameters and ~40% slower training for marginal accuracy gains\n",
        "\n",
        "5. **GIN Expressiveness**:\n",
        "   - **Theoretical strength**: Maximally expressive (can distinguish any graph structure)\n",
        "   - **Practical performance**: Similar to other architectures on this task\n",
        "   - **Risk**: Higher capacity may lead to overfitting on smaller graphs without careful regularization\n",
        "\n",
        "**Architecture Selection Guidelines:**\n",
        "\n",
        "| Graph Property | Recommended Architecture |\n",
        "|----------------|--------------------------|\n",
        "| High homophily (>60%) | **GCN** - Simple and effective |\n",
        "| Heterogeneous edges | **GAT** - Learn adaptive weights |\n",
        "| Inductive learning needed | **GraphSAGE** - Generalizes to new nodes |\n",
        "| Complex topology | **GIN** - Maximum expressiveness |\n",
        "| Limited compute | **GCN** - Fastest training |\n",
        "\n",
        "**Why High Homophily Matters:**\n",
        "When 68% of edges connect same-class nodes (as in GitHub), even simple averaging (GCN) captures strong community signals. Attention mechanisms (GAT) and complex aggregation (GIN) provide diminishing returns because there are few \"noisy\" neighbors to filter out.\n",
        "\n",
        "**Production Recommendation:**\n",
        "For GitHub-like social networks with high homophily:\n",
        "1. **Start with GCN** (simplest, fastest, effective)\n",
        "2. **Try GraphSAGE** if you need to predict on new users\n",
        "3. **Consider GAT** only if accuracy gains justify 40% training slowdown\n",
        "4. **Use GIN** for theoretical guarantees or complex structural patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Baseline Model Comparisons\n",
        "\n",
        "Now we compare the GNN performance against traditional machine learning baselines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw2k3UhZS9ks"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97yIzR5D53i1"
      },
      "source": [
        "**Making Dataframe out of encoded data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "MqTADPpyIUpb"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[117], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_encoded\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:119\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     arrays, refs \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:629\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    626\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(val)\n\u001b[0;32m    627\u001b[0m     val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m--> 629\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m com\u001b[38;5;241m.\u001b[39mrequire_length_match(val, index)\n\u001b[0;32m    631\u001b[0m refs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\construction.py:654\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    651\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m _try_cast(data, dtype, copy)\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 654\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subarr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    656\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\dtypes\\cast.py:130\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    127\u001b[0m arr: ArrayLike\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mrange\u001b[39m)):\n\u001b[1;32m--> 130\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_1d_object_array_from_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# The caller is responsible for ensuring that we have np.ndarray\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m#  or ExtensionArray here.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     arr \u001b[38;5;241m=\u001b[39m values\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\dtypes\\cast.py:1599\u001b[0m, in \u001b[0;36mconstruct_1d_object_array_from_listlike\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1581\u001b[0m \u001b[38;5;124;03mTransform any list-like object in a 1-dimensional numpy array of object\u001b[39;00m\n\u001b[0;32m   1582\u001b[0m \u001b[38;5;124;03mdtype.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;124;03m1-dimensional numpy array of dtype object\u001b[39;00m\n\u001b[0;32m   1596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;66;03m# numpy will try to interpret nested lists as further dimensions, hence\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;66;03m# making a 1D array that contains list-likes is a bit tricky:\u001b[39;00m\n\u001b[1;32m-> 1599\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1600\u001b[0m result[:] \u001b[38;5;241m=\u001b[39m values\n\u001b[0;32m   1601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "X_encoded = pd.DataFrame(data_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEhOwEyYIesP"
      },
      "outputs": [],
      "source": [
        "y = target_df.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXH7pIjeK06y"
      },
      "outputs": [],
      "source": [
        "transposed_df = X_encoded.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "OpGYmzZfLgIt",
        "outputId": "ba4bfbd6-568b-443c-d7c6-a48bae70b805"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>3995</th>\n",
              "      <th>3996</th>\n",
              "      <th>3997</th>\n",
              "      <th>3998</th>\n",
              "      <th>3999</th>\n",
              "      <th>4000</th>\n",
              "      <th>4001</th>\n",
              "      <th>4002</th>\n",
              "      <th>4003</th>\n",
              "      <th>4004</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 4005 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0     1     2     3     4     5     6     7     8     9     ...  3995  \\\n",
              "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
              "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
              "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
              "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
              "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
              "\n",
              "   3996  3997  3998  3999  4000  4001  4002  4003  4004  \n",
              "0     0     0     0     0     0     0     0     0     0  \n",
              "1     0     0     0     0     0     0     0     0     0  \n",
              "2     0     0     0     0     0     0     0     0     0  \n",
              "3     0     0     0     0     0     0     0     1     0  \n",
              "4     0     0     0     0     0     0     0     0     0  \n",
              "\n",
              "[5 rows x 4005 columns]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transposed_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PudsENFl53i2"
      },
      "source": [
        "**Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx9cGkuKCtNr",
        "outputId": "da1784e6-fb7a-4870-c78d-0cd79b000cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy in iteration 1 is 44.22%\n",
            "Accuracy in iteration 2 is 43.56%\n",
            "Accuracy in iteration 2 is 43.56%\n",
            "Accuracy in iteration 3 is 45.35%\n",
            "Accuracy in iteration 3 is 45.35%\n",
            "Accuracy in iteration 4 is 44.65%\n",
            "Average accuracy: 44.45%\n",
            "\n",
            "Confusion Matrix for Test Data:\n",
            "Accuracy in iteration 4 is 44.65%\n",
            "Average accuracy: 44.45%\n",
            "\n",
            "Confusion Matrix for Test Data:\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Split the data using 4-Fold Cross Validation\n",
        "kf = KFold(n_splits=4)\n",
        "accuracy_scores = []\n",
        "confusion_matrices = []  # To store confusion matrices for each fold\n",
        "\n",
        "# Counter variable to keep track of each fold\n",
        "k = 1\n",
        "\n",
        "for train_index, test_index in kf.split(y):\n",
        "    # Split the data into training and testing sets based on the current fold\n",
        "    X_train, X_test = transposed_df.iloc[train_index], transposed_df.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Define the classifier\n",
        "    naive_bayes_classifier = GaussianNB()\n",
        "    naive_bayes_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = naive_bayes_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    confusion_test = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Print accuracy for the current iteration\n",
        "    print(f\"Accuracy in iteration {k} is {accuracy * 100:.2f}%\")\n",
        "    k += 1\n",
        "\n",
        "    # Append the accuracy to the list\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    # Append the confusion matrix to the list\n",
        "    confusion_matrices.append(confusion_test)\n",
        "\n",
        "# Calculate the average accuracy\n",
        "avg_acc = sum(accuracy_scores) / len(accuracy_scores)\n",
        "\n",
        "# Calculate the average confusion matrix\n",
        "avg_confusion_matrix = sum(confusion_matrices) / len(confusion_matrices)\n",
        "\n",
        "print(f'Average accuracy: {avg_acc * 100:.2f}%')\n",
        "\n",
        "# Calculate the average confusion matrix as integers\n",
        "avg_confusion_matrix_int = avg_confusion_matrix.astype(int)\n",
        "print()\n",
        "# Print the average confusion matrix\n",
        "print(\"Confusion Matrix for Test Data:\")\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(avg_confusion_matrix_int, annot=True, fmt='d', cmap='viridis',\n",
        "            xticklabels=range(2), yticklabels=range(2))\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix -Naive Bayes')\n",
        "save_fig('confusion_naivebayes.png')\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkFQm2vn53i6",
        "outputId": "038bc786-b0d4-4ab3-c073-dd2088bdb6cb"
      },
      "outputs": [],
      "source": [
        "# Filter data based on conditions for the second dataset\n",
        "y_test_0_lr = len(y_test[y_test == 0])\n",
        "y_test_1_lr = len(y_test[y_test == 1])\n",
        "y_pred_0_lr = len(y_pred[y_pred == 0])\n",
        "y_pred_1_lr = len(y_pred[y_pred == 1])\n",
        "\n",
        "conditions = [\"Web Actual\", \"ML Actual\", \"Web Predicted\", \"ML Actual\"]\n",
        "colors = ['b', 'b', 'g', 'g']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-k9I1Ho53i6"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKO-Kxp7NAyU",
        "outputId": "7f2465d8-c61f-4fac-91e6-5c508caa332a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy in iteration 1 is 83.07%\n",
            "\n",
            "Accuracy in iteration 2 is 83.62%\n",
            "\n",
            "Accuracy in iteration 2 is 83.62%\n",
            "\n",
            "Accuracy in iteration 3 is 83.28%\n",
            "\n",
            "Accuracy in iteration 3 is 83.28%\n",
            "\n",
            "Accuracy in iteration 4 is 83.72%\n",
            "\n",
            "Average accuracy: 83.42%\n",
            "Confusion Matrix for Test Data:\n",
            "Accuracy in iteration 4 is 83.72%\n",
            "\n",
            "Average accuracy: 83.42%\n",
            "Confusion Matrix for Test Data:\n"
          ]
        }
      ],
      "source": [
        "\n",
        "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
        "accuracy_scores_logisticRegression = []\n",
        "confusion_matrices_lr = []  # Initialize the confusion matrices list\n",
        "\n",
        "k = 1\n",
        "\n",
        "for train_index, test_index in kf.split(transposed_df):  # Changed 'y' to 'transposed_df'\n",
        "    X_train, X_test = transposed_df.iloc[train_index], transposed_df.iloc[test_index]\n",
        "    y_train, y_test_lr = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    logisticRegression = LogisticRegression(max_iter=1000, solver='liblinear', C=0.1, class_weight='balanced', penalty='l1')\n",
        "    logisticRegression.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_lr = logisticRegression.predict(X_test)\n",
        "    accuracy_logisticRegression = accuracy_score(y_test_lr, y_pred_lr)\n",
        "    confusion_test_lr = confusion_matrix(y_test_lr, y_pred_lr)\n",
        "\n",
        "    # Append the confusion matrix to the list\n",
        "    confusion_matrices_lr.append(confusion_test_lr)\n",
        "\n",
        "    print(f\"Accuracy in iteration {k} is {accuracy_logisticRegression * 100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    accuracy_scores_logisticRegression.append(accuracy_logisticRegression)\n",
        "\n",
        "    k += 1\n",
        "\n",
        "avg_acc_logisticRegression = np.mean(accuracy_scores_logisticRegression)  # Use np.mean to calculate the average\n",
        "\n",
        "print(f'Average accuracy: {avg_acc_logisticRegression * 100:.2f}%')\n",
        "\n",
        "# Calculate the average confusion matrix\n",
        "avg_confusion_matrix_lr = np.mean(confusion_matrices_lr, axis=0)  # Use np.mean to calculate the average\n",
        "\n",
        "# Calculate the average confusion matrix as integers\n",
        "avg_confusion_matrix_int_lr = avg_confusion_matrix_lr.astype(int)\n",
        "\n",
        "# Print the average confusion matrix\n",
        "print(\"Confusion Matrix for Test Data:\")\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(avg_confusion_matrix_int_lr, annot=True, fmt='d', cmap='viridis',\n",
        "            xticklabels=range(2), yticklabels=range(2))\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix -Logistic Regression')\n",
        "save_fig('confusion_logistic.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruV3uG0O53i7",
        "outputId": "b89cc2da-4fe5-4400-8ab3-8d49da211c4f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Filter data based on conditions for the second dataset\n",
        "y_test_0_lr = len(y_test_lr[y_test_lr == 0])\n",
        "y_test_1_lr = len(y_test_lr[y_test_lr == 1])\n",
        "y_pred_0_lr = len(y_pred_lr[y_pred_lr == 0])\n",
        "y_pred_1_lr = len(y_pred_lr[y_pred_lr == 1])\n",
        "\n",
        "conditions = [\"Web Actual\", \"ML Actual\", \"Web Predicted\", \"ML Actual\"]\n",
        "colors = ['b', 'b', 'g', 'g']\n",
        "\n",
        "bar_width = 0.5\n",
        "x = [i for i in range(len(conditions))]\n",
        "counts = [y_test_0_lr, y_test_1_lr, y_pred_0_lr, y_pred_1_lr]\n",
        "\n",
        "plt.figure(figsize=(4, 6))  # Adjust the figure size as needed\n",
        "\n",
        "# Create the bar chart with the same width and appearance\n",
        "plt.bar(x, counts, color=colors, width=bar_width, alpha=0.4)\n",
        "plt.xticks(x, conditions, rotation=45)\n",
        "plt.xlabel('Condition')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Actual Labels VS Predicted Labels -Logistic Regression')\n",
        "plt.tight_layout()\n",
        "save_fig('actual_vs_predicted_lr.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW5QnJ1l53i7"
      },
      "source": [
        "**Comparing Accuracies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Osjwi3C53i7",
        "outputId": "fcb0936e-cb1f-4952-f4b3-61bcb2fdb908",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Average CV accuracies\n",
        "avg_acc_nb = np.mean(accuracy_scores)\n",
        "avg_acc_lr = np.mean(accuracy_scores_logisticRegression)\n",
        "\n",
        "# GNN: take the last recorded test accuracy from the last training run\n",
        "gnn_test_acc = test_list[-1]\n",
        "\n",
        "models = [\"Naive Bayes\", \"Logistic Regression\", \"SocialGNN\"]\n",
        "avg_accuracies = [avg_acc_nb, avg_acc_lr, gnn_test_acc]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(models, avg_accuracies)\n",
        "plt.ylim(0.0, 1.0)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model comparison (average accuracy)\")\n",
        "save_fig('model_comparison.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HykMiLDO5775"
      },
      "source": [
        "Reference:\n",
        "\n",
        "- Awadelrahman. (2021, July 13). Tutorial Graph Neural networks on social networks. Kaggle. https://www.kaggle.com/code/awadelrahman/tutorial-graph-neural-networks-on-social-networks\n",
        "- Awan, A. A. (2022, July 21). A Comprehensive Introduction to Graph Neural Networks (GNNs). https://www.datacamp.com/tutorial/comprehensive-introduction-graph-neural-networks-gnns-tutorial\n",
        "- DARPAtv. (2017, February 15). A DARPA perspective on Artificial intelligence [Video]. YouTube. https://www.youtube.com/watch?v=-O01G3tSYpU\n",
        "- Khare, P. (2023, August 8). Unravelling Node2Vec: A Guide to Node Embeddings with Python Implementation. Medium. https://medium.com/illumination/unravelling-node2vec-a-guide-to-node-embeddings-with-python-implementation-c131603153bd\n",
        "- PyG Documentation — pytorch_geometric  documentation. (n.d.). https://pytorch-geometric.readthedocs.io/en/latest/index.html#\n",
        "- SNAP: Network datasets: Social circles. (n.d.). https://snap.stanford.edu/data/github-social.html\n",
        "-TensorFlow. (2021, June 17). Intro to graph neural networks (ML Tech Talks) [Video]. YouTube. https://www.youtube.com/watch?v=8owQBFAHw7E\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nHNhO6bEhKa"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
